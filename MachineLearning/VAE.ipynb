{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lPWQiw0XNNL"
      },
      "source": [
        "# VAE\n",
        "目的：\n",
        "- 输入：MNIST 手写数字图像（28x28）\n",
        "- 编码器 → 学到隐变量 z（概率分布）\n",
        "- 解码器 → 从 z 重建图像\n",
        "最后可生成新图像、学习潜在空间结构   \n",
        "\n",
        "整体流程：   \n",
        "1. 先定义模型结构 VAE\n",
        "2. 然后定义损失函数 loss_function\n",
        "3. 接着写训练代码 for epoch in ...（通常写在 main 里或者 notebook 后半部分）\n",
        "4. 在 model(x) 这一步自动触发 forward()（PyTorch 会自动调用）\n",
        "5. 用返回的值算 loss → 用 loss 来训练模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UVIQ9pf3XRJC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这里的`input_dim=784`就是根据MNIST手写数据图像的尺寸来确定的\n",
        "## 1.  定义VAE模型\n",
        "### 1.1 编码器结构\n",
        "第一步：图像（784维） → 隐藏层（400维）\n",
        "然后：输出两个东西 → `μ` 和 `log(σ²)`（代表分布）\n",
        "### 1.2 解码器结构\n",
        "从 latent 向量 z → 还原图像（输出784维）\n",
        "sigmoid 保证输出像素值在 `[0,1]`\n",
        "z（20维）\n",
        " ↓ fc2（20→400）+ ReLU\n",
        "隐藏层 h（400维）\n",
        " ↓ fc3（400→784）+ Sigmoid\n",
        "还原图像（784维）\n",
        "\n",
        "### 1.3 重参数操作\n",
        "你想象一下我们写个网络这样：\n",
        "```python\n",
        "mu, logvar = encoder(x)\n",
        "z = torch.normal(mu, sigma)  # ← 你以为可以这么搞\n",
        "x_hat = decoder(z)\n",
        "```\n",
        "这样写在数学上没法对 mu 和 sigma 求导数，也就是你没法告诉 encoder “你输出的 mu 应该改成啥”。这样网络就没法训练！   \n",
        "\n",
        "我们用重参数技巧（Reparameterization Trick）来绕开这个问题, `z=μ+σ⋅ϵ,ϵ∼N(0,1)`。   \n",
        "从 𝑁(μ,σ^2) 采样 𝑧不合适，为啥不直接从 N(μ,σ^2) 里采样 z？\n",
        "因为直接采样会导致梯度无法传播，`reparameterization trick` 通过引入一个随机变量ϵ来解决这个问题。\n",
        "- ϵ是从标准正态分布N(0,1)中采样的随机变量, `ϵ∼N(0,1)`，然后通过公式 z=μ+σ⋅ϵ 来生成潜在变量z。\n",
        "- 这里的σ是通过对数方差logvar计算得到的，具体为 `σ=exp(0.5*logvar)`。\n",
        "- 这里的μ是编码器输出的均值\n",
        "这样就可以将随机性引入到模型中，同时又能保持梯度的可传播性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdcqdhnGXnSO"
      },
      "outputs": [],
      "source": [
        "from logging import NullHandler\n",
        "# 1. 定义VAE模型\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        # 编码器部分\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        # 解码器部分\n",
        "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    # encoder: 输入数据x，输出均值mu和对数方差logvar\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    # reparameterize 重参数技巧: 使用均值和对数方差生成潜在变量z\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    # decode: 使用潜在变量z生成重构数据\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc2(z))\n",
        "        return torch.sigmoid(self.fc3(h))\n",
        "\n",
        "    # forward: 整体前向传播过程，返回重构数据、均值和对数方差\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 重参数小例子\n",
        "运行这个例子会看到如下结果：   \n",
        "mu: [1.0, -0.5]   \n",
        "std: [1.0, 0.1353]        ← 注意 std 是从 logvar 还原出来的   \n",
        "epsilon: [0.62, -0.18]    ← 随机生成的 ε   \n",
        "z: [1.62, -0.5243]        ← z = mu + std * eps   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 模拟编码器输出\n",
        "mu = torch.tensor([1.0, -0.5])           # 均值\n",
        "logvar = torch.tensor([0.0, -2.0])       # 对数方差\n",
        "\n",
        "# 重参数技巧实现 z = mu + sigma * epsilon\n",
        "std = torch.exp(0.5 * logvar)            # 方差开根号 = 标准差\n",
        "eps = torch.randn_like(std)             # 从 N(0,1) 采样 ε\n",
        "z = mu + std * eps                      # 最终采样出的潜变量 z\n",
        "\n",
        "print(\"mu:\", mu.tolist())\n",
        "print(\"std:\", std.tolist())\n",
        "print(\"epsilon:\", eps.tolist())\n",
        "print(\"z:\", z.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. 定义VAE的损失函数（重建 + KL散度）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I4Q0QyN9Xpuf"
      },
      "outputs": [],
      "source": [
        "# 2. 定义VAE的损失函数（重建 + KL散度）\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ghne76y9XrZN"
      },
      "outputs": [],
      "source": [
        "# 3. 准备训练\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnZvYJh8XuZr",
        "outputId": "9f80ee20-b185-4704-f949-8543cb163e78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.61MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 4. 加载MNIST数据\n",
        "transform = transforms.ToTensor()\n",
        "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAZcBOR-tu4h",
        "outputId": "f2de87a2-79db-4f4a-e0f8-fd5a69a101fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 163.2387\n",
            "Epoch 2, Loss: 120.6043\n",
            "Epoch 3, Loss: 114.2044\n",
            "Epoch 4, Loss: 111.3464\n",
            "Epoch 5, Loss: 109.6627\n"
          ]
        }
      ],
      "source": [
        "# 5. 训练模型\n",
        "model.train()\n",
        "for epoch in range(1, 6):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.view(-1, 784).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtl0HTL_XzCS",
        "outputId": "c3892c81-46b0-4882-886c-4e3ee4839b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "新图像已保存为 vae_output/sample.png\n"
          ]
        }
      ],
      "source": [
        "# 6. 随机生成新图像\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(64, 20).to(device)\n",
        "    sample = model.decode(z).cpu().view(64, 1, 28, 28)\n",
        "    os.makedirs(\"vae_output\", exist_ok=True)\n",
        "    save_image(sample, 'vae_output/sample.png')\n",
        "    print(\"新图像已保存为 vae_output/sample.png\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

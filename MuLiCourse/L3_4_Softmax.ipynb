{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 交叉熵是啥？\n",
    "\n",
    "可以把交叉熵（Cross-Entropy）理解成一个**“判断你有多蠢”**的函数，衡量的是：你的模型在预测分类的时候，**和真实答案偏离了多少**。\n",
    "\n",
    "更具体点：\n",
    "- 如果你非常自信地预测错了（比如你模型预测“这绝对是猫”，结果其实是狗），交叉熵的惩罚就会很重。\n",
    "- 如果你预测得接近真实情况（比如你说“70%可能是猫，30%可能是狗”，结果真的是猫），交叉熵的惩罚就小一些。\n",
    "- 如果你完全预测正确而且很确定（比如说“100%是猫”，结果真的是猫），那交叉熵就几乎为0，表示你预测得完美。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ 那 softmax 是干嘛的？\n",
    "\n",
    "softmax 是一个**把模型输出变成概率**的函数。\n",
    "\n",
    "假设你模型最后输出了一堆分数（logits），像这样：\n",
    "\n",
    "```\n",
    "[3.1, 1.2, 0.3] → 看起来是乱七八糟的数\n",
    "```\n",
    "\n",
    "softmax 会把它们变成概率：\n",
    "\n",
    "```\n",
    "[0.84, 0.11, 0.05] → 表示模型认为第一类最可能\n",
    "```\n",
    "\n",
    "所以 softmax 是一个“翻译器”，把模型生硬的分数翻译成“我觉得这三类的可能性分别是多少”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 它俩啥关系？\n",
    "\n",
    "两者经常**搭配使用**，主要是在分类问题中：\n",
    "\n",
    "1. **softmax** 把模型输出变成概率分布（就像投票结果）。\n",
    "2. **交叉熵** 拿这些概率去和“正确答案”比一比（看看你错得有多离谱）。\n",
    "\n",
    "比如在训练神经网络时，最后一步通常是：\n",
    "```python\n",
    "loss = cross_entropy(softmax(logits), ground_truth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ 小结一句话：\n",
    "\n",
    "> softmax 把模型输出转换成“你觉得哪个对”的概率；交叉熵则来判断你“到底错得有多狠”。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
